{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcQPAsrOZ2-p"
      },
      "source": [
        "# Building a tokenizer, block by block"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we’ve seen in the previous sections, tokenization comprises several **steps**:\n",
        "\n",
        "- `Normalization` (any cleanup of the text that is deemed necessary, such as removing spaces or accents, Unicode normalization, etc.)\n",
        "- `Pre-tokenization` (splitting the input into words)\n",
        "- Running the input `through the model` (using the pre-tokenized words to produce a sequence of tokens)\n",
        "- `Post-processing` (adding the special tokens of the tokenizer, generating the attention mask and token type IDs)\n",
        "\n",
        "The `🤗 Tokenizers library` has been built to provide several options for each of those steps, which you can mix and match together.\n",
        "\n",
        "In this section we’ll see how we can `build a tokenizer from scratch`, as opposed to training a new tokenizer from an old one as we did in section 2.\n",
        "\n",
        "You’ll then be able to **build any kind of tokenizer you can think of**!"
      ],
      "metadata": {
        "id": "8qToOHOvaMMF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to use Tokenizers library API ?\n",
        "\n",
        "- see the list of whole components in https://huggingface.co/docs/tokenizers/components"
      ],
      "metadata": {
        "id": "-67PadWwbCZ2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okM7MM-TZ2-r"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWPkHgmpZ2-r"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Acquiring a corpus\n",
        "To train our new tokenizer, we will use a small corpus of text (so the examples run fast). The steps for acquiring the corpus are similar to the ones we took at the beginning of this chapter, but this time we’ll use the WikiText-2 dataset:\n",
        "\n",
        "- The function `get_training_corpus()` is a generator that will yield batches of 1,000 texts, which we will use to train the tokenizer."
      ],
      "metadata": {
        "id": "O-nRt9GobcGs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vineXXqNZ2-s"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"train\")\n",
        "\n",
        "\n",
        "def get_training_corpus():\n",
        "    for i in range(0, len(dataset), 1000):\n",
        "        yield dataset[i : i + 1000][\"text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🤗 Tokenizers can also be trained on text files directly. Here’s how we can generate a text file containing all the texts/inputs from WikiText-2 that we can use locally:"
      ],
      "metadata": {
        "id": "ODgCX00ubo4L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVWqbOYjZ2-t"
      },
      "outputs": [],
      "source": [
        "with open(\"wikitext-2.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for i in range(len(dataset)):\n",
        "        f.write(dataset[i][\"text\"] + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we’ll show you how to build your own BERT, GPT-2, and XLNet tokenizers, block by block.\n",
        "\n",
        "That will give us an example of each of the **three main tokenization algorithms: WordPiece, BPE, and Unigram**. Let’s start with **BERT!**"
      ],
      "metadata": {
        "id": "cGiJCFjabs3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a WordPiece tokenizer from scratch\n",
        "To build a tokenizer with the 🤗 Tokenizers library, we start by instantiating a `Tokenizer object` with a `model, then set its normalizer, pre_tokenizer, post_processor, and decoder attributes` to the values we want.\n",
        "\n",
        "For this example, we’ll create a Tokenizer with a **WordPiece model**:"
      ],
      "metadata": {
        "id": "DHXKZ33Gb0j_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_4PWULLZ2-t"
      },
      "outputs": [],
      "source": [
        "from tokenizers import (\n",
        "    decoders,\n",
        "    models,\n",
        "    normalizers,\n",
        "    pre_tokenizers,\n",
        "    processors,\n",
        "    trainers,\n",
        "    Tokenizer,\n",
        ")\n",
        "\n",
        "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have to specify the `unk_token` so the model knows what to return when it encounters characters it hasn’t seen before. Other arguments we can set here include the vocab of our model (we’re going to train the model, so we don’t need to set this) and `max_input_chars_per_word`, which specifies a maximum length for each word (words longer than the value passed will be split)."
      ],
      "metadata": {
        "id": "3Os0odWZcCYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalization\n",
        "\n",
        "The first step of tokenization is **normalization**, so let’s begin with that. Since BERT is widely used, there is a `BertNormalizer` with the classic options we can set for BERT: `lowercase` and `strip_accents`, which are self-explanatory; `clean_text` to remove all control characters and replace repeating spaces with a single one; and `handle_chinese_chars`, which places spaces around Chinese characters.\n",
        "\n",
        "To replicate the bert-base-uncased tokenizer, we can just set this normalizer:"
      ],
      "metadata": {
        "id": "P_rdvJpqcMgG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3j5TOllZ2-t"
      },
      "outputs": [],
      "source": [
        "tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generally speaking, however, when building a new tokenizer you won’t have access to such a handy normalizer already implemented in the 🤗 Tokenizers library — so let’s see how to `create the BERT normalizer by hand`.\n",
        "\n",
        "The library provides a Lowercase normalizer and a StripAccents normalizer, and you can compose several normalizers using a Sequence:"
      ],
      "metadata": {
        "id": "ST-03sChcqzM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TNQj50pZ2-u"
      },
      "outputs": [],
      "source": [
        "tokenizer.normalizer = normalizers.Sequence(\n",
        "    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’re also using an **NFD** Unicode normalizer, as otherwise the `StripAccents` normalizer won’t properly recognize the accented characters and thus won’t strip them out.\n",
        "\n",
        "use the `normalize_str()` method of the normalizer to check out the effects it has on a given text:"
      ],
      "metadata": {
        "id": "7hk9ldyEc2Fo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POitxF1AZ2-u",
        "outputId": "63d2f250-d89a-4ddf-8d85-de060fb9daca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "hello how are u?"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-tokenization\n",
        "\n",
        "Next is the `pre-tokenization step`. Again, there is a prebuilt BertPreTokenizer that we can use:"
      ],
      "metadata": {
        "id": "1D-SJKE6dLGK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KS9pLcIZ2-v"
      },
      "outputs": [],
      "source": [
        "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or we can build it from scratch:\n",
        "\n",
        "- Note that the `Whitespace` pre-tokenizer splits on whitespace and all characters that are not letters, digits, or the underscore character, so it technically **splits on whitespace and punctuation**:"
      ],
      "metadata": {
        "id": "fk6pCCumdNQA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdAfF68kZ2-v"
      },
      "outputs": [],
      "source": [
        "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2gHxg7cZ2-v",
        "outputId": "842a8c97-280b-489a-b1d5-59e151be7b7f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Let', (0, 3)), (\"'\", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)),\n",
              " ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you **only want to split on whitespace**, you should use the `WhitespaceSplit` pre-tokenizer instead:"
      ],
      "metadata": {
        "id": "tu385EuIdgOE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnKOQZ58Z2-w",
        "outputId": "8eac8c75-d012-4dba-f480-c532720f4443"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(\"Let's\", (0, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre-tokenizer.', (14, 28))]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
        "pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like with `normalizers`, you can use a `Sequence` to compose **several pre-tokenizers**:"
      ],
      "metadata": {
        "id": "xKZ9vXeTdp3e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pd4wR-x9Z2-w",
        "outputId": "3c78f737-ed56-4a4e-ee53-3048fcb7dd6b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Let', (0, 3)), (\"'\", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)),\n",
              " ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pre_tokenizer = pre_tokenizers.Sequence(\n",
        "    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]\n",
        ")\n",
        "pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train (Run inputs through model)\n",
        "\n",
        "The next step in the `tokenization pipeline` is running the inputs through the model. We already specified our model in the initialization, but we still need to train it, which will require a `WordPieceTrainer`.\n",
        "\n",
        "The main thing to remember when instantiating a trainer in 🤗 Tokenizers is that you `need to pass it all the special tokens` you intend to use — otherwise it won’t add them to the vocabulary, since they are not in the training corpus:\n",
        "\n",
        "- As well as specifying the `vocab_size` and `special_tokens`, we can set the `min_frequency` (the number of times a token must appear to be included in the vocabulary) or change the `continuing_subword_prefix` (if we want to use something different from ##).\n",
        "\n"
      ],
      "metadata": {
        "id": "JHrYARbcdxDf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppIx1z1LZ2-w"
      },
      "outputs": [],
      "source": [
        "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
        "trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train our model using the **iterator** we defined earlier, we just have to execute this command:"
      ],
      "metadata": {
        "id": "bMoaD_BOeOIf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_S_cH2QZ2-w"
      },
      "outputs": [],
      "source": [
        "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also use **text files** to train our tokenizer, which would look like this (we reinitialize the model with an empty WordPiece beforehand):"
      ],
      "metadata": {
        "id": "FmfmOss2eUUx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUHT8sFWZ2-w"
      },
      "outputs": [],
      "source": [
        "tokenizer.model = models.WordPiece(unk_token=\"[UNK]\")\n",
        "tokenizer.train([\"wikitext-2.txt\"], trainer=trainer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In both cases, we can then **test the tokenizer** on a text by calling the `encode()` method:\n",
        "\n",
        "- The encoding obtained is an `Encoding`, which contains all the necessary outputs of the tokenizer in its various attributes: `ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, and overflowing`."
      ],
      "metadata": {
        "id": "wIPz_GNje_Xy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM0WGFJBZ2-w",
        "outputId": "2ad377dd-688a-4349-e7c4-f2f5d0ef5c54"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '.']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
        "print(encoding.tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Post-processing\n",
        "\n",
        "The last step in the tokenization pipeline is `post-processing`.\n",
        "\n",
        "We need to add the [CLS] token at the beginning and the [SEP] token at the end (or after each sentence, if we have a pair of sentences).\n",
        "\n",
        "We will use a `TemplateProcessor` for this, but first we need to **know the IDs** of the [CLS] and [SEP] tokens in the vocabulary:"
      ],
      "metadata": {
        "id": "M3GIDZdCfRCn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVZIIAziZ2-w",
        "outputId": "9a285dcd-919e-4398-d126-ec84217c67eb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2, 3)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
        "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
        "print(cls_token_id, sep_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To write the template for the `TemplateProcessor`, we have to specify how to treat a single sentence and a pair of sentences.\n",
        "\n",
        "For both, we write the special tokens we want to use; the first (or single) sentence is represented by `$A`, while the second sentence (if encoding a pair) is represented by `$B`.\n",
        "\n",
        "For each of these (special tokens and sentences), we also specify the corresponding token type ID after a colon.\n",
        "\n",
        "The classic **BERT** template is thus defined as follows:\n",
        "\n"
      ],
      "metadata": {
        "id": "1LtTH3wnfiMe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f43cTJ7fZ2-w"
      },
      "outputs": [],
      "source": [
        "tokenizer.post_processor = processors.TemplateProcessing(\n",
        "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
        "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
        "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### encode"
      ],
      "metadata": {
        "id": "bWWQzPB2gTr3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nisml8ruZ2-w",
        "outputId": "5c402eec-9f6e-46c0-96f1-07d3c07e81c1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['[CLS]', 'let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '.', '[SEP]']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
        "print(encoding.tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPUzNvhpZ2-w",
        "outputId": "bb9e153e-8901-4e39-ecdc-0fdf8407b488"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['[CLS]', 'let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '...', '[SEP]', 'on', 'a', 'pair', 'of', 'sentences', '.', '[SEP]']\n",
              "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoding = tokenizer.encode(\"Let's test this tokenizer...\", \"on a pair of sentences.\")\n",
        "print(encoding.tokens)\n",
        "print(encoding.type_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### decoder\n",
        "\n",
        "We’ve almost finished building this tokenizer from scratch — the last step is to include a `decoder`:"
      ],
      "metadata": {
        "id": "xN0lE19YgGPj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2fupnEmZ2-w"
      },
      "outputs": [],
      "source": [
        "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7xkMs9TZ2-w",
        "outputId": "c612e3d3-bc4a-4517-b8fa-9192e872e52e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"let's test this tokenizer... on a pair of sentences.\""
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(encoding.ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save our tokenizer"
      ],
      "metadata": {
        "id": "ry6pMf-Kggkc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74JcaWcoZ2-w"
      },
      "outputs": [],
      "source": [
        "tokenizer.save(\"tokenizer.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then reload that file in a Tokenizer object with the `from_file()` method:"
      ],
      "metadata": {
        "id": "y9OJ22-BglTm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNA6nlM3Z2-w"
      },
      "outputs": [],
      "source": [
        "new_tokenizer = Tokenizer.from_file(\"tokenizer.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use your own tokenizer\n",
        "\n",
        "To use this tokenizer in 🤗 Transformers, we have to wrap it in a `PreTrainedTokenizerFast`.\n",
        "\n",
        "We can either use the generic class or, if our tokenizer corresponds to an existing model, use that class (here, `BertTokenizerFast`).\n",
        "\n",
        "If you apply this lesson to build a brand new tokenizer, you will have to use the first option.\n",
        "\n",
        "To wrap the tokenizer in a `PreTrainedTokenizerFast`, we can either pass the tokenizer we built as a `tokenizer_object` or pass the tokenizer file we saved as `tokenizer_file`.\n",
        "- The key thing to remember is that we have to manually set all the special tokens, since that class can’t infer from the tokenizer object which token is the mask token, the [CLS] token, etc.:"
      ],
      "metadata": {
        "id": "IkH4i-jFgrxq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qe64y4cZ2-x"
      },
      "outputs": [],
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_object=tokenizer,\n",
        "    # tokenizer_file=\"tokenizer.json\", # You can load from the tokenizer file, alternatively\n",
        "    unk_token=\"[UNK]\",\n",
        "    pad_token=\"[PAD]\",\n",
        "    cls_token=\"[CLS]\",\n",
        "    sep_token=\"[SEP]\",\n",
        "    mask_token=\"[MASK]\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMhp11jeZ2-x"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizerFast\n",
        "\n",
        "wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a BPE tokenizer from scratch"
      ],
      "metadata": {
        "id": "gJa1wHeZhS0C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whsW4d7oZ2-x"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(models.BPE())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also like for BERT, we could initialize this model with a vocabulary if we had one (we would need to pass the vocab and merges in this case), but since we will train from scratch, we don’t need to do that.\n",
        "\n",
        "We also don’t need to specify an `unk_token` because `GPT-2 uses byte-level BPE`, which doesn’t require it.\n",
        "\n",
        "`GPT-2 does not use a normalizer`, so we skip that step and go directly to the pre-tokenization:\n"
      ],
      "metadata": {
        "id": "uPCsMxk8hZmK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Skip normalization. Pre-tokenization."
      ],
      "metadata": {
        "id": "-r_QOAX7iV2A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47j3dUiaZ2-x"
      },
      "outputs": [],
      "source": [
        "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zS00veeZ2-x",
        "outputId": "640f2058-9ddd-4a8a-8e20-d59068b04e3b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Let', (0, 3)), (\"'s\", (3, 5)), ('Ġtest', (5, 10)), ('Ġpre', (10, 14)), ('-', (14, 15)),\n",
              " ('tokenization', (15, 27)), ('!', (27, 28))]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test pre-tokenization!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train BPE"
      ],
      "metadata": {
        "id": "c3r7mVJXicSQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1ZeQ0NRZ2-x"
      },
      "outputs": [],
      "source": [
        "trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=[\"<|endoftext|>\"])\n",
        "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8Lhdr7IZ2-x"
      },
      "outputs": [],
      "source": [
        "tokenizer.model = models.BPE()\n",
        "tokenizer.train([\"wikitext-2.txt\"], trainer=trainer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Post-processing"
      ],
      "metadata": {
        "id": "cgd4xUuuihcJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seIqmHcgZ2-x",
        "outputId": "5260b14d-3272-4351-d1d0-9e631915521c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['L', 'et', \"'\", 's', 'Ġtest', 'Ġthis', 'Ġto', 'ken', 'izer', '.']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
        "print(encoding.tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1GYCtLeZ2-x"
      },
      "outputs": [],
      "source": [
        "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNRMblaiZ2-x",
        "outputId": "0c0d7672-e7ba-48ae-8030-379595156eb8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' test'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence = \"Let's test this tokenizer.\"\n",
        "encoding = tokenizer.encode(sentence)\n",
        "start, end = encoding.offsets[4]\n",
        "sentence[start:end]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xQ9qH_aZ2-x"
      },
      "outputs": [],
      "source": [
        "tokenizer.decoder = decoders.ByteLevel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hme1ohMJZ2-x",
        "outputId": "9fa1ba7f-f3de-44af-d4ef-c23b85ec8431"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Let's test this tokenizer.\""
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(encoding.ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## use it in 🤗 Transformers (2 options)"
      ],
      "metadata": {
        "id": "G4YI8_PkivzX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8olIdqsZ2-x"
      },
      "outputs": [],
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_object=tokenizer,\n",
        "    bos_token=\"<|endoftext|>\",\n",
        "    eos_token=\"<|endoftext|>\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wqD0s3WZ2-1"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2TokenizerFast\n",
        "\n",
        "wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Unigram tokenizer from scratch\n",
        "\n",
        "Let’s now build an XLNet tokenizer.\n",
        "\n",
        "Like for the previous tokenizers, we start by initializing a Tokenizer with a Unigram model:"
      ],
      "metadata": {
        "id": "xcqN8pk0iBoD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jTlyzDWZ2-1"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(models.Unigram())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalization\n",
        "Again, we could initialize this model with a vocabulary if we had one.\n",
        "\n",
        "For the normalization, XLNet uses a few replacements (which come from SentencePiece):\n",
        "\n",
        "- This replaces “ and ” with ” and any sequence of two or more spaces with a single space, as well as removing the accents in the texts to tokenize."
      ],
      "metadata": {
        "id": "h5_nkcgQiLR5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhwbDnwbZ2-1"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Regex\n",
        "\n",
        "tokenizer.normalizer = normalizers.Sequence(\n",
        "    [\n",
        "        normalizers.Replace(\"``\", '\"'),\n",
        "        normalizers.Replace(\"''\", '\"'),\n",
        "        normalizers.NFKD(),\n",
        "        normalizers.StripAccents(),\n",
        "        normalizers.Replace(Regex(\" {2,}\"), \" \"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-tokenizer\n",
        "\n",
        "The pre-tokenizer to use for any **SentencePiece tokenizer** is Metaspace: `Metaspace()`"
      ],
      "metadata": {
        "id": "pHkwebCwjT-F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4mY8lYJZ2-1"
      },
      "outputs": [],
      "source": [
        "tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTuDM8wHZ2-1",
        "outputId": "05857749-ba31-4fd6-fbfe-d0722aa2e148"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(\"▁Let's\", (0, 5)), ('▁test', (5, 10)), ('▁the', (10, 14)), ('▁pre-tokenizer!', (14, 29))]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test the pre-tokenizer!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train\n",
        "\n",
        "Next is the model, which needs training. XLNet has quite a few special tokens:\n",
        "\n",
        "- A very important argument not to forget for the `UnigramTrainer` is the `unk_token`.\n",
        "\n",
        "  We can also pass along other arguments specific to the Unigram algorithm, such as the `shrinking_factor` for each step where we remove tokens (defaults to 0.75) or the `max_piece_length` to specify the maximum length of a given token (defaults to 16).\n",
        "\n"
      ],
      "metadata": {
        "id": "jBQFAQ8Gjqui"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcmOFqkbZ2-1"
      },
      "outputs": [],
      "source": [
        "special_tokens = [\"<cls>\", \"<sep>\", \"<unk>\", \"<pad>\", \"<mask>\", \"<s>\", \"</s>\"]\n",
        "trainer = trainers.UnigramTrainer(\n",
        "    vocab_size=25000, special_tokens=special_tokens, unk_token=\"<unk>\"\n",
        ")\n",
        "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnovHgPzZ2-1"
      },
      "outputs": [],
      "source": [
        "tokenizer.model = models.Unigram()\n",
        "tokenizer.train([\"wikitext-2.txt\"], trainer=trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANXjgtCpZ2-1",
        "outputId": "79f631a7-a1d7-4399-bfb2-f4c93dc834cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['▁Let', \"'\", 's', '▁test', '▁this', '▁to', 'ken', 'izer', '.']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
        "print(encoding.tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Post-processing\n",
        "A peculiarity of XLNet is that it puts the `<cls>` token at the end of the sentence, with a type ID of 2 (to distinguish it from the other tokens).\n",
        "\n",
        "It’s padding on the left, as a result. We can deal with all the special tokens and token type IDs with a template, like for BERT, but first we have to get the IDs of the `<cls>` and `<sep>` tokens:"
      ],
      "metadata": {
        "id": "C0XYFVYrkO3n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQT2JaN8Z2-1",
        "outputId": "5987aa26-0507-4c85-88a0-6e93d399f307"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0 1"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cls_token_id = tokenizer.token_to_id(\"<cls>\")\n",
        "sep_token_id = tokenizer.token_to_id(\"<sep>\")\n",
        "print(cls_token_id, sep_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZyCD_nEZ2-1"
      },
      "outputs": [],
      "source": [
        "tokenizer.post_processor = processors.TemplateProcessing(\n",
        "    single=\"$A:0 <sep>:0 <cls>:2\",\n",
        "    pair=\"$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2\",\n",
        "    special_tokens=[(\"<sep>\", sep_token_id), (\"<cls>\", cls_token_id)],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWpTmJjTZ2-1",
        "outputId": "96d2059b-8960-47e4-e02f-702b25e9effa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['▁Let', \"'\", 's', '▁test', '▁this', '▁to', 'ken', 'izer', '.', '.', '.', '<sep>', '▁', 'on', '▁', 'a', '▁pair', \n",
              "  '▁of', '▁sentence', 's', '!', '<sep>', '<cls>']\n",
              "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoding = tokenizer.encode(\"Let's test this tokenizer...\", \"on a pair of sentences!\")\n",
        "print(encoding.tokens)\n",
        "print(encoding.type_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we add a `Metaspace decoder`:"
      ],
      "metadata": {
        "id": "iBhqLRjMklyD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTOUfw6FZ2-1"
      },
      "outputs": [],
      "source": [
        "tokenizer.decoder = decoders.Metaspace()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## use it in 🤗 Transformers (2 options)"
      ],
      "metadata": {
        "id": "nR2JHkcjkr_5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lN1nAU0OZ2-1"
      },
      "outputs": [],
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_object=tokenizer,\n",
        "    bos_token=\"<s>\",\n",
        "    eos_token=\"</s>\",\n",
        "    unk_token=\"<unk>\",\n",
        "    pad_token=\"<pad>\",\n",
        "    cls_token=\"<cls>\",\n",
        "    sep_token=\"<sep>\",\n",
        "    mask_token=\"<mask>\",\n",
        "    padding_side=\"left\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMzJlG5eZ2-2"
      },
      "outputs": [],
      "source": [
        "from transformers import XLNetTokenizerFast\n",
        "\n",
        "wrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you have seen how the various building blocks are used to build existing tokenizers, you should be able to write any tokenizer you want with the 🤗 Tokenizers library and be able to use it in 🤗 Transformers."
      ],
      "metadata": {
        "id": "R9o04joEk7xh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EO2AdgDuk8HE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Building a tokenizer, block by block",
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}