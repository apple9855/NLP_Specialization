{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSwClAi5Y-mI"
      },
      "source": [
        "# Semantic search with FAISS (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8coqzOLY-mK"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRvLrz9VY-mK"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]\n",
        "!pip install faiss-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lchw5eVgY-mL",
        "outputId": "1c5815da-17b3-49bc-fcc9-9e837181138f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_request'],\n",
              "    num_rows: 2855\n",
              "})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "issues_dataset = load_dataset(\"lewtun/github-issues\", split=\"train\")\n",
        "issues_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we’ve specified the default train split in `load_dataset()`, so it returns a Dataset instead of a DatasetDict.\n",
        "\n",
        "**The first order of business is to filter out the pull requests, as these tend to be rarely used for answering user queries and will introduce noise in our search engine.**\n",
        "\n",
        "As should be familiar by now, we can use the `Dataset.filter()` function to exclude these rows in our dataset. While we’re at it, let’s also filter out rows with no comments, since these provide no answers to user queries:"
      ],
      "metadata": {
        "id": "BtlrNoTEaTQP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPb70lSJY-mL",
        "outputId": "166b9fb7-1fcc-4f7f-bb5c-56c426d4346d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_request'],\n",
              "    num_rows: 771\n",
              "})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "issues_dataset = issues_dataset.filter(\n",
        "    lambda x: (x[\"is_pull_request\"] == False and len(x[\"comments\"]) > 0)\n",
        ")\n",
        "issues_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that there are a lot of columns in our dataset, most of which we don’t need to build our search engine. From a search perspective, the most informative columns are title, body, and comments, while html_url provides us with a link back to the source issue. Let’s use the Dataset.`remove_columns()` function to drop the rest:"
      ],
      "metadata": {
        "id": "3_-lTEaVabn7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ouHyETWY-mL",
        "outputId": "aeaec6b8-c885-42bd-c130-58da11dd6080"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['html_url', 'title', 'comments', 'body'],\n",
              "    num_rows: 771\n",
              "})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "columns = issues_dataset.column_names\n",
        "columns_to_keep = [\"title\", \"body\", \"html_url\", \"comments\"]\n",
        "columns_to_remove = set(columns_to_keep).symmetric_difference(columns)\n",
        "issues_dataset = issues_dataset.remove_columns(columns_to_remove)\n",
        "issues_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoSiEbhYY-mL"
      },
      "outputs": [],
      "source": [
        "issues_dataset.set_format(\"pandas\")\n",
        "df = issues_dataset[:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMWUPoU5Y-mM",
        "outputId": "c3937cc1-fa12-4d39-a941-267f2da87ead"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['the bug code locate in ：\\r\\n    if data_args.task_name is not None:\\r\\n        # Downloading and loading a dataset from the hub.\\r\\n        datasets = load_dataset(\"glue\", data_args.task_name, cache_dir=model_args.cache_dir)',\n",
              " 'Hi @jinec,\\r\\n\\r\\nFrom time to time we get this kind of `ConnectionError` coming from the github.com website: https://raw.githubusercontent.com\\r\\n\\r\\nNormally, it should work if you wait a little and then retry.\\r\\n\\r\\nCould you please confirm if the problem persists?',\n",
              " 'cannot connect，even by Web browser，please check that  there is some  problems。',\n",
              " 'I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem...']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"comments\"][0].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create our embeddings we’ll augment each comment with the issue’s `title` and `body`, since these fields often include useful *contextual information*.\n",
        "\n",
        "Because our comments column is currently a list of comments for each issue, we need to *“explode”* the column so that each row consists of an (html_url, title, body, comment) tuple.\n",
        "\n",
        "In Pandas we can do this with the `DataFrame.explode() function`, which creates a new row for each element in a list-like column, while replicating all the other column values. To see this in action, let’s first switch to the Pandas DataFrame format:\n",
        "\n"
      ],
      "metadata": {
        "id": "RpxlACoCaj69"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GO_z4OqoY-mM"
      },
      "outputs": [],
      "source": [
        "comments_df = df.explode(\"comments\", ignore_index=True)\n",
        "comments_df.head(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great, we can see the rows have been replicated, with the comments column containing the individual comments! Now that we’re finished with Pandas, we can quickly switch back to a Dataset by loading the DataFrame in memory:"
      ],
      "metadata": {
        "id": "pVRXVOBsa1S5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnj-kZSGY-mM",
        "outputId": "bd817a38-a1a8-446a-efc0-46ae9d744f14"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['html_url', 'title', 'comments', 'body'],\n",
              "    num_rows: 2842\n",
              "})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "comments_dataset = Dataset.from_pandas(comments_df)\n",
        "comments_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayAb6zPdY-mM"
      },
      "outputs": [],
      "source": [
        "comments_dataset = comments_dataset.map(\n",
        "    lambda x: {\"comment_length\": len(x[\"comments\"].split())}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have one comment per row, let’s create a new `comments_length` column that contains the number of words per comment:"
      ],
      "metadata": {
        "id": "Eb4QeamKbAES"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btCbD7tMY-mM",
        "outputId": "575f3caf-bc94-4388-b1c2-a6462716b75e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],\n",
              "    num_rows: 2098\n",
              "})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "comments_dataset = comments_dataset.filter(lambda x: x[\"comment_length\"] > 15)\n",
        "comments_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having cleaned up our dataset a bit, let’s concatenate the issue title, description, and comments together in a *new text column*. As usual, we’ll write a simple function that we can pass to` Dataset.map()`:"
      ],
      "metadata": {
        "id": "L-R0dTCzbGZl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWxTKc_EY-mM"
      },
      "outputs": [],
      "source": [
        "def concatenate_text(examples):\n",
        "    return {\n",
        "        \"text\": examples[\"title\"]\n",
        "        + \" \\n \"\n",
        "        + examples[\"body\"]\n",
        "        + \" \\n \"\n",
        "        + examples[\"comments\"]\n",
        "    }\n",
        "\n",
        "\n",
        "comments_dataset = comments_dataset.map(concatenate_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating text embeddings**\n",
        "\n",
        "\n",
        "We saw in Chapter 2 that we can obtain token embeddings by using the `AutoModel` class. All we need to do is pick a suitable checkpoint to load the model from. Fortunately, there’s a library called `sentence-transformers` that is dedicated to creating embeddings.\n",
        "https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search\n",
        "\n",
        "As described in the library’s documentation, our use case is an example of `asymmetric semantic search` because we have a short query whose answer we’d like to find in a longer document, like a an issue comment.\n",
        "\n",
        " The handy [model overview table](https://www.sbert.net/docs/pretrained_models.html#model-overview) in the documentation indicates that the `multi-qa-mpnet-base-dot-v1` checkpoint has the best performance for semantic search, so we’ll use that for our application. We’ll also load the tokenizer using the same checkpoint:"
      ],
      "metadata": {
        "id": "i5Oj1bDYbRyx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daX-XJ0_Y-mM"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "model = AutoModel.from_pretrained(model_ckpt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMZD6TksY-mN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we mentioned earlier, we’d like to represent each entry in our GitHub issues corpus as a single vector, so we need to “pool” or average our token embeddings in some way. One popular approach is to perform CLS pooling on our model’s outputs, where we simply collect the last hidden state for the special `[CLS]` token. The following function does the trick for us://"
      ],
      "metadata": {
        "id": "PiFAKO-6eAuq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKxJeQswY-mN"
      },
      "outputs": [],
      "source": [
        "def cls_pooling(model_output):\n",
        "    return model_output.last_hidden_state[:, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we’ll create a helper function that will tokenize a list of documents, place the tensors on the GPU, feed them to the model, and finally apply CLS pooling to the outputs:"
      ],
      "metadata": {
        "id": "yR59KyBpeKbH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlisfYl5Y-mN"
      },
      "outputs": [],
      "source": [
        "def get_embeddings(text_list):\n",
        "    encoded_input = tokenizer(\n",
        "        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n",
        "    )\n",
        "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
        "    model_output = model(**encoded_input)\n",
        "    return cls_pooling(model_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can test the function works by feeding it the first text entry in our corpus and inspecting the output shape:"
      ],
      "metadata": {
        "id": "IZmR14x0eQKs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfxCILaFY-mN",
        "outputId": "f5cfed45-ce62-4c6f-8ffb-2004fa2c7028"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 768])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding = get_embeddings(comments_dataset[\"text\"][0])\n",
        "embedding.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great, we’ve converted the first entry in our corpus into a 768-dimensional vector! We can use `Dataset.map()` to apply our `get_embeddings()` function to each row in our corpus, so let’s create a new embeddings column as follows:"
      ],
      "metadata": {
        "id": "U8rEV4xzeXYY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Phl_VT3aY-mN"
      },
      "outputs": [],
      "source": [
        "embeddings_dataset = comments_dataset.map(\n",
        "    lambda x: {\"embeddings\": get_embeddings(x[\"text\"]).detach().cpu().numpy()[0]}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that we’ve converted the embeddings to NumPy arrays — that’s because 🤗 Datasets requires this format when we try to index them with FAISS, which we’ll do next."
      ],
      "metadata": {
        "id": "c_WZY855eljN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using FAISS for efficient similarity search**\n",
        "\n",
        "\n",
        "Now that we have a dataset of embeddings, we need some way to search over them. To do this, we’ll use a *special data structure* in 🤗 Datasets called a **FAISS index**.\n",
        "\n",
        "**FAISS** (short for Facebook AI Similarity Search) is a library that provides efficient algorithms to quickly search and cluster embedding vectors.\n",
        "\n",
        "The basic idea behind FAISS is to create a special data structure called an index that allows one to find which embeddings are similar to an input embedding. Creating a FAISS index in 🤗 Datasets is simple — we use the `Dataset.add_faiss_index()` function and specify which column of our dataset we’d like to index:"
      ],
      "metadata": {
        "id": "wvS00tHxepGn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-eoLg9UY-mN"
      },
      "outputs": [],
      "source": [
        "embeddings_dataset.add_faiss_index(column=\"embeddings\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now perform queries on this index by doing a nearest neighbor lookup with the `Dataset.get_nearest_examples()` function. Let’s test this out by first embedding a question as follows:"
      ],
      "metadata": {
        "id": "wETimoWcfSOf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsAhv9u1Y-mN",
        "outputId": "0efe999d-d917-43bc-e17e-b7dcc40cf2c0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 768])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question = \"How can I load a dataset offline?\"\n",
        "question_embedding = get_embeddings([question]).cpu().detach().numpy()\n",
        "question_embedding.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgBQXkgqY-mN"
      },
      "outputs": [],
      "source": [
        "scores, samples = embeddings_dataset.get_nearest_examples(\n",
        "    \"embeddings\", question_embedding, k=5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-BIddWFY-mN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "samples_df = pd.DataFrame.from_dict(samples)\n",
        "samples_df[\"scores\"] = scores\n",
        "samples_df.sort_values(\"scores\", ascending=False, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E56RXD2zY-mN",
        "outputId": "964862ba-4130-4a4f-99b5-e59b4194e122"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\"\"\n",
              "COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if offline mode is added similar to how `transformers` loads models offline fine.\n",
              "\n",
              "@mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?\n",
              "SCORE: 25.505046844482422\n",
              "TITLE: Discussion using datasets in offline mode\n",
              "URL: https://github.com/huggingface/datasets/issues/824\n",
              "==================================================\n",
              "\n",
              "COMMENT: The local dataset builders (csv, text , json and pandas) are now part of the `datasets` package since #1726 :)\n",
              "You can now use them offline\n",
              "\\`\\`\\`python\n",
              "datasets = load_dataset(\"text\", data_files=data_files)\n",
              "\\`\\`\\`\n",
              "\n",
              "We'll do a new release soon\n",
              "SCORE: 24.555509567260742\n",
              "TITLE: Discussion using datasets in offline mode\n",
              "URL: https://github.com/huggingface/datasets/issues/824\n",
              "==================================================\n",
              "\n",
              "COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there's no internet.\n",
              "\n",
              "Let me know if you know other ways that can make the offline mode experience better. I'd be happy to add them :)\n",
              "\n",
              "I already note the \"freeze\" modules option, to prevent local modules updates. It would be a cool feature.\n",
              "\n",
              "----------\n",
              "\n",
              "> @mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?\n",
              "\n",
              "Indeed `load_dataset` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.\n",
              "For example if you have a dataset script at `./my_dataset/my_dataset.py` then you can do\n",
              "\\`\\`\\`python\n",
              "load_dataset(\"./my_dataset\")\n",
              "\\`\\`\\`\n",
              "and the dataset script will generate your dataset once and for all.\n",
              "\n",
              "----------\n",
              "\n",
              "About I'm looking into having `csv`, `json`, `text`, `pandas` dataset builders already included in the `datasets` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.\n",
              "cf #1724\n",
              "SCORE: 24.14896583557129\n",
              "TITLE: Discussion using datasets in offline mode\n",
              "URL: https://github.com/huggingface/datasets/issues/824\n",
              "==================================================\n",
              "\n",
              "COMMENT: > here is my way to load a dataset offline, but it **requires** an online machine\n",
              ">\n",
              "> 1. (online machine)\n",
              ">\n",
              "> ```\n",
              ">\n",
              "> import datasets\n",
              ">\n",
              "> data = datasets.load_dataset(...)\n",
              ">\n",
              "> data.save_to_disk(/YOUR/DATASET/DIR)\n",
              ">\n",
              "> ```\n",
              ">\n",
              "> 2. copy the dir from online to the offline machine\n",
              ">\n",
              "> 3. (offline machine)\n",
              ">\n",
              "> ```\n",
              ">\n",
              "> import datasets\n",
              ">\n",
              "> data = datasets.load_from_disk(/SAVED/DATA/DIR)\n",
              ">\n",
              "> ```\n",
              ">\n",
              ">\n",
              ">\n",
              "> HTH.\n",
              "\n",
              "\n",
              "SCORE: 22.893993377685547\n",
              "TITLE: Discussion using datasets in offline mode\n",
              "URL: https://github.com/huggingface/datasets/issues/824\n",
              "==================================================\n",
              "\n",
              "COMMENT: here is my way to load a dataset offline, but it **requires** an online machine\n",
              "1. (online machine)\n",
              "\\`\\`\\`\n",
              "import datasets\n",
              "data = datasets.load_dataset(...)\n",
              "data.save_to_disk(/YOUR/DATASET/DIR)\n",
              "\\`\\`\\`\n",
              "2. copy the dir from online to the offline machine\n",
              "3. (offline machine)\n",
              "\\`\\`\\`\n",
              "import datasets\n",
              "data = datasets.load_from_disk(/SAVED/DATA/DIR)\n",
              "\\`\\`\\`\n",
              "\n",
              "HTH.\n",
              "SCORE: 22.406635284423828\n",
              "TITLE: Discussion using datasets in offline mode\n",
              "URL: https://github.com/huggingface/datasets/issues/824\n",
              "==================================================\n",
              "\"\"\""
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for _, row in samples_df.iterrows():\n",
        "    print(f\"COMMENT: {row.comments}\")\n",
        "    print(f\"SCORE: {row.scores}\")\n",
        "    print(f\"TITLE: {row.title}\")\n",
        "    print(f\"URL: {row.html_url}\")\n",
        "    print(\"=\" * 50)\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Semantic search with FAISS (PyTorch)",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}