{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2RwM3-5Rhmm"
      },
      "source": [
        "# Translation (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let‚Äôs now dive into translation. This is another `sequence-to-sequence` task, which means it‚Äôs a problem that can be formulated as going from one sequence to another. In that sense the problem is pretty close to `summarization`, and you could adapt what we will see here to other sequence-to-sequence problems such as:\n",
        "\n",
        "- Style transfer: Creating a model that translates texts written in a certain style to another (e.g., formal to casual or Shakespearean English to modern English)\n",
        "- Generative question answering: Creating a model that generates answers to questions, given a context\n"
      ],
      "metadata": {
        "id": "-dV_Lr_dRnj6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwkeLAkTRhmn"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-r5JZwzLRhmn"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]\n",
        "!pip install accelerate\n",
        "# To run the training on TPU, you will need to uncomment the following line:\n",
        "# !pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
        "!apt install git-lfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X96MuXf1Rhmo"
      },
      "source": [
        "You will need to setup git, adapt your email and name in the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qf5hEl-ORhmo"
      },
      "outputs": [],
      "source": [
        "!git config --global user.email \"you@example.com\"\n",
        "!git config --global user.name \"Your Name\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g4c00aURhmo"
      },
      "source": [
        "You will also need to be logged in to the Hugging Face Hub. Execute the following and enter your credentials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TmJPIIERhmp"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we will fine-tune a Marian model pretrained to translate from English to French (since a lot of Hugging Face employees speak both those languages) on the `KDE4 dataset`, which is a dataset of localized files for the *KDE apps*.\n",
        "\n",
        "The model we will use has been pretrained on a large corpus of French and English texts taken from the Opus dataset, which actually contains the KDE4 dataset. But even if the pretrained model we use has seen that data during its pretraining, we will see that we can get a better version of it after fine-tuning.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Preparing the data\n",
        "To fine-tune or train a translation model from scratch, we will need a dataset suitable for the task. As mentioned previously, we‚Äôll use the KDE4 dataset in this section, but you can adapt the code to use your own data quite easily, as long as you have pairs of sentences in the two languages you want to translate from and into. Refer back to Chapter 5 if you need a reminder of how to load your custom data in a Dataset."
      ],
      "metadata": {
        "id": "Qh4a2qhCSfgb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwJMYGOMRhmp"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset(\"kde4\", lang1=\"en\", lang2=\"fr\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zO-1MddRhmp",
        "outputId": "12c03312-59f2-4bbd-94be-b9b57efb95d4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'translation'],\n",
              "        num_rows: 210173\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_b9OTYURhmq",
        "outputId": "b8a5ce60-b0f9-4ba3-ef25-531e87d3950a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'translation'],\n",
              "        num_rows: 189155\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'translation'],\n",
              "        num_rows: 21018\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "split_datasets = raw_datasets[\"train\"].train_test_split(train_size=0.9, seed=20)\n",
        "split_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqN3jyvsRhmq"
      },
      "outputs": [],
      "source": [
        "split_datasets[\"validation\"] = split_datasets.pop(\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-KeZLD-Rhmq",
        "outputId": "81de1527-c141-49cf-91a1-f805ea4ab085"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'en': 'Default to expanded threads',\n",
              " 'fr': 'Par d√©faut, d√©velopper les fils de discussion'}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "split_datasets[\"train\"][1][\"translation\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We get a dictionary with two sentences in the pair of languages we requested. One particularity of this dataset full of technical computer science terms is that they are all fully translated in French.\n",
        "\n",
        "However, French engineers leave most computer science-specific words in English when they talk. Here, for instance, the word ‚Äúthreads‚Äù might well appear in a French sentence, especially in a technical conversation; but in this dataset it has been translated into the more correct ‚Äúfils de discussion.‚Äù\n",
        "\n",
        "The pretrained model we use, which has been pretrained on a larger corpus of French and English sentences, takes the easier option of leaving the word as is:"
      ],
      "metadata": {
        "id": "8wcTH-F9TF7f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hk5PZiMmRhmq",
        "outputId": "bddc5019-1b69-458b-8f3e-f855fbd9ccf0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'translation_text': 'Par d√©faut pour les threads √©largis'}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
        "translator = pipeline(\"translation\", model=model_checkpoint)\n",
        "translator(\"Default to expanded threads\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0IMxcFCRhmr",
        "outputId": "600ac254-2363-4ab9-ed73-37904362cdc0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'en': 'Unable to import %1 using the OFX importer plugin. This file is not the correct format.',\n",
              " 'fr': \"Impossible d'importer %1 en utilisant le module d'extension d'importation OFX. Ce fichier n'a pas un format correct.\"}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "split_datasets[\"train\"][172][\"translation\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLiW_MTkRhmr",
        "outputId": "a1b22740-ce35-481b-c0de-b67abceaaa68"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'translation_text': \"Impossible d'importer %1 en utilisant le plugin d'importateur OFX. Ce fichier n'est pas le bon format.\"}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translator(\n",
        "    \"Unable to import %1 using the OFX importer plugin. This file is not the correct format.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Processing the data\n",
        "You should know the drill by now: the texts all need to be converted into sets of token IDs so the model can make sense of them. For this task, we‚Äôll need to tokenize both the inputs and the targets. Our first task is to create our tokenizer object. As noted earlier, we‚Äôll be using a Marian English to French pretrained model.\n",
        "\n",
        "If you are trying this code with another pair of languages, make sure to adapt the model checkpoint. The `Helsinki-NLP` organization provides more than a thousand models in multiple languages."
      ],
      "metadata": {
        "id": "agq9js75Tae0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEz6rgihRhmr"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*üí° If you are using a multilingual tokenizer such as mBART, mBART-50, or M2M100, you will need to set the language codes of your inputs and targets in the tokenizer by setting `tokenizer.src_lang` and `tokenizer.tgt_lang` to the right values.*\n",
        "\n",
        "The preparation of our data is pretty straightforward. There‚Äôs just one thing to remember; you need to ensure that the tokenizer processes the targets in the output language (here, French). You can do this by passing the targets to the `text_targets` argument of the tokenizer‚Äôs `__call__ `method."
      ],
      "metadata": {
        "id": "0qtstoYsU1fH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFV4NfgZRhmr",
        "outputId": "200e7e32-79db-4b24-eb44-09e6336989b3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [47591, 12, 9842, 19634, 9, 0], 'attention_mask': [1, 1, 1, 1, 1, 1], 'labels': [577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "en_sentence = split_datasets[\"train\"][1][\"translation\"][\"en\"]\n",
        "fr_sentence = split_datasets[\"train\"][1][\"translation\"][\"fr\"]\n",
        "\n",
        "inputs = tokenizer(en_sentence, text_target=fr_sentence)\n",
        "inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAKNh-IBRhmr",
        "outputId": "9d53c275-f200-4dbc-efa3-dbc02ab40ce1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['‚ñÅPar', '‚ñÅd√©', 'f', 'aut', ',', '‚ñÅd√©', 've', 'lop', 'per', '‚ñÅles', '‚ñÅfil', 's', '‚ñÅde', '‚ñÅdiscussion', '</s>']\n",
              "['‚ñÅPar', '‚ñÅd√©faut', ',', '‚ñÅd√©velopper', '‚ñÅles', '‚ñÅfils', '‚ñÅde', '‚ñÅdiscussion', '</s>']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wrong_targets = tokenizer(fr_sentence)\n",
        "print(tokenizer.convert_ids_to_tokens(wrong_targets[\"input_ids\"]))\n",
        "print(tokenizer.convert_ids_to_tokens(inputs[\"labels\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIRQK7WHRhmr"
      },
      "outputs": [],
      "source": [
        "max_length = 128\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
        "    targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n",
        "    model_inputs = tokenizer(\n",
        "        inputs, text_target=targets, max_length=max_length, truncation=True\n",
        "    )\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHdX5UObRhmr"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = split_datasets.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=split_datasets[\"train\"].column_names,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚ö†Ô∏è We don‚Äôt pay attention to the attention mask of the targets, as the model won‚Äôt expect it. Instead, the labels corresponding to a padding token should be set to -100 so they are ignored in the loss computation. This will be done by our data collator later on since we are applying dynamic padding, but if you use padding here, you should adapt the preprocessing function to set all labels that correspond to the padding token to -100."
      ],
      "metadata": {
        "id": "2P_HDUxlYBGh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning the model with the Trainer API\n",
        "The actual code using the Trainer will be the same as before, with just one little change: we use a `Seq2SeqTrainer` here, which is a subclass of Trainer that will allow us to properly deal with the evaluation, using the `generate()` method to predict outputs from the inputs. We‚Äôll dive into that in more detail when we talk about the metric computation.\n",
        "\n",
        "First things first, we need an actual model to fine-tune. We‚Äôll use the usual AutoModel API:"
      ],
      "metadata": {
        "id": "QkLGIiKlYJKB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeQnrOTwRhmr"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data collation\n",
        "We‚Äôll need a data collator to deal with the padding for dynamic batching. We can‚Äôt just use a `DataCollatorWithPadding` like in Chapter 3 in this case, because that only pads the inputs (input IDs, attention mask, and token type IDs). Our labels should also be padded to the maximum length encountered in the labels. And, as mentioned previously, the padding value used to pad the labels should be -100 and not the padding token of the tokenizer, to make sure those padded values are ignored in the loss computation.\n",
        "\n",
        "This is all done by a `DataCollatorForSeq2Seq`. Like the `DataCollatorWithPadding`, it takes the tokenizer used to preprocess the inputs, but it also takes the model. This is because this data collator will also be responsible for preparing the decoder input IDs, which are shifted versions of the labels with a special token at the beginning.\n",
        "\n",
        "Since this shift is done slightly differently for different architectures, the DataCollatorForSeq2Seq needs to know the model object:"
      ],
      "metadata": {
        "id": "-4ZVG9WkYZQd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcNI4-sWRhmr"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHqHpP0eRhmr",
        "outputId": "524b2ee9-d400-4ec4-c63a-0dc6cef64ddf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['attention_mask', 'input_ids', 'labels', 'decoder_input_ids'])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(1, 3)])\n",
        "batch.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04QYvE5KRhmr",
        "outputId": "a9d525bf-c16f-4cd3-f17a-2e165ebe74be"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[  577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100],\n",
              "        [ 1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,   817,\n",
              "           550,  7032,  5821,  7907, 12649,     0]])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch[\"labels\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWC4FMdjRhmr",
        "outputId": "f05b1f17-c5be-4c88-9cea-70e088b2c6e6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[59513,   577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,\n",
              "         59513, 59513, 59513, 59513, 59513, 59513],\n",
              "        [59513,  1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,\n",
              "           817,   550,  7032,  5821,  7907, 12649]])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch[\"decoder_input_ids\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKtt59xuRhmr",
        "outputId": "013a4ce7-2925-4c1f-d3f6-238d4ec43135"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]\n",
              "[1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for i in range(1, 3):\n",
        "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics\n",
        "\n",
        "The feature that `Seq2SeqTrainer` adds to its superclass `Trainer` is the ability to use the `generate()` method during evaluation or prediction. During training, the model will use the `decoder_input_ids` with an attention mask ensuring it does not use the tokens after the token it‚Äôs trying to predict, to speed up training. During inference we won‚Äôt be able to use those since we won‚Äôt have labels, so it‚Äôs a good idea to evaluate our model with the same setup.\n",
        "\n",
        "As we saw in Chapter 1, the decoder performs inference by predicting tokens one by one ‚Äî something that‚Äôs implemented behind the scenes in ü§ó Transformers by the generate() method. The `Seq2SeqTrainer` will let us use that method for evaluation if we set `predict_with_generate=True`.\n",
        "\n",
        "The *traditional metric used for translation* is the `BLEU score`, introduced in a 2002 article by Kishore Papineni et al. The BLEU score evaluates how close the translations are to their labels. It does not measure the intelligibility or grammatical correctness of the model‚Äôs generated outputs, but uses statistical rules to ensure that all the words in the generated outputs also appear in the targets. In addition, there are rules that penalize repetitions of the same words if they are not also repeated in the targets (to avoid the model outputting sentences like \"the the the the the\") and output sentences that are shorter than those in the targets (to avoid the model outputting sentences like \"the\").\n",
        "\n",
        "*One weakness with BLEU* is that it expects the text to already be tokenized, which makes it difficult to compare scores between models that use different tokenizers.\n",
        "\n",
        "So instead, the most commonly used metric for benchmarking translation models today is `SacreBLEU`, which addresses this weakness (and others) by standardizing the tokenization step. To use this metric, we first need to install the `SacreBLEU library`:\n",
        "\n",
        "- The score can go from 0 to 100, and higher is better."
      ],
      "metadata": {
        "id": "qSH7N2ygbuWA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nE0tFlp_Rhmr"
      },
      "outputs": [],
      "source": [
        "!pip install sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scTY8WxLRhmr"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"sacrebleu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oca1SzIdRhms",
        "outputId": "c0e0c698-ca64-42af-fd81-fe3a26a7951d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'score': 46.750469682990165,\n",
              " 'counts': [11, 6, 4, 3],\n",
              " 'totals': [12, 11, 10, 9],\n",
              " 'precisions': [91.67, 54.54, 40.0, 33.33],\n",
              " 'bp': 0.9200444146293233,\n",
              " 'sys_len': 12,\n",
              " 'ref_len': 13}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions = [\n",
        "    \"This plugin lets you translate web pages between several languages automatically.\"\n",
        "]\n",
        "references = [\n",
        "    [\n",
        "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
        "    ]\n",
        "]\n",
        "metric.compute(predictions=predictions, references=references)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This gets a BLEU score of 46.75, which is rather good ‚Äî for reference, the original Transformer model in the ‚ÄúAttention Is All You Need‚Äù paper achieved a BLEU score of 41.8 on a similar translation task between English and French!\n",
        "\n",
        "(For more information about the individual metrics, like counts and bp, see the [SacreBLEU repository](https://github.com/mjpost/sacrebleu/blob/078c440168c6adc89ba75fe6d63f0d922d42bcfe/sacrebleu/metrics/bleu.py#L74).) On the other hand, if we try with the two bad types of predictions (lots of repetitions or too short) that often come out of translation models, we will get rather bad BLEU scores:"
      ],
      "metadata": {
        "id": "7avuA-GEeJYq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFnEhdPhRhms",
        "outputId": "b9fdc80d-4ce4-47cf-8a1a-f82a2ad8f189"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'score': 1.683602693167689,\n",
              " 'counts': [1, 0, 0, 0],\n",
              " 'totals': [4, 3, 2, 1],\n",
              " 'precisions': [25.0, 16.67, 12.5, 12.5],\n",
              " 'bp': 0.10539922456186433,\n",
              " 'sys_len': 4,\n",
              " 'ref_len': 13}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions = [\"This This This This\"]\n",
        "references = [\n",
        "    [\n",
        "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
        "    ]\n",
        "]\n",
        "metric.compute(predictions=predictions, references=references)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9fBAKSPRhms",
        "outputId": "2cdf5f83-098b-4473-b079-14ee88287da1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'score': 0.0,\n",
              " 'counts': [2, 1, 0, 0],\n",
              " 'totals': [2, 1, 0, 0],\n",
              " 'precisions': [100.0, 100.0, 0.0, 0.0],\n",
              " 'bp': 0.004086771438464067,\n",
              " 'sys_len': 2,\n",
              " 'ref_len': 13}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions = [\"This plugin\"]\n",
        "references = [\n",
        "    [\n",
        "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
        "    ]\n",
        "]\n",
        "metric.compute(predictions=predictions, references=references)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get from the model outputs to texts the metric can use, we will use the `tokenizer.batch_decode() `method.\n",
        "\n",
        "We just have to clean up all the -100s in the labels (the tokenizer will automatically do the same for the padding token):"
      ],
      "metadata": {
        "id": "htFiCoCre2Wm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlX8-9TdRhms"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    # In case the model returns more than the prediction logits\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100s in the labels as we can't decode them\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
        "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    return {\"bleu\": result[\"score\"]}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning the model"
      ],
      "metadata": {
        "id": "UXLErkdMeq7n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2D3gbSqRhms"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgcQeMxIRhmt"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    f\"marian-finetuned-kde4-en-to-fr\",\n",
        "    evaluation_strategy=\"no\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=64,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=3,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    push_to_hub=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LFSEjLjRhmt"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O06XJDe0Rhmt",
        "outputId": "ef2c405e-214f-4533-8ead-463f741a3263"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 1.6964408159255981,\n",
              " 'eval_bleu': 39.26865061007616,\n",
              " 'eval_runtime': 965.8884,\n",
              " 'eval_samples_per_second': 21.76,\n",
              " 'eval_steps_per_second': 0.341}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.evaluate(max_length=max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPZ453GiRhmt"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANJxcWgGRhmt",
        "outputId": "498a5bde-cc9d-4ac4-9b99-4c671d0442b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.8558505773544312,\n",
              " 'eval_bleu': 52.94161337775576,\n",
              " 'eval_runtime': 714.2576,\n",
              " 'eval_samples_per_second': 29.426,\n",
              " 'eval_steps_per_second': 0.461,\n",
              " 'epoch': 3.0}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.evaluate(max_length=max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aex0thJQRhmt",
        "outputId": "5ee0613e-4df1-4444-8227-c270a3042595"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'https://huggingface.co/sgugger/marian-finetuned-kde4-en-to-fr/commit/3601d621e3baae2bc63d3311452535f8f58f6ef3'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.push_to_hub(tags=\"translation\", commit_message=\"Training complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A custom training loop\n",
        "Let‚Äôs now take a look at the full training loop, so you can easily customize the parts you need. It will look a lot like what we did in section 2 and Chapter 3.\n",
        "\n",
        "## Preparing everything for training\n",
        "You‚Äôve seen all of this a few times now, so we‚Äôll go through the code quite quickly. First we‚Äôll build the DataLoaders from our datasets, after setting the datasets to the \"torch\" format so we get PyTorch tensors:"
      ],
      "metadata": {
        "id": "RvWLCVdnfhUw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYKuyTs_Rhmw"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"train\"],\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=8,\n",
        ")\n",
        "eval_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"validation\"], collate_fn=data_collator, batch_size=8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLnHa5quRhmw"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7dG5RH0Rhmw"
      },
      "outputs": [],
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEbwpl3VRhmw"
      },
      "outputs": [],
      "source": [
        "from accelerate import Accelerator\n",
        "\n",
        "accelerator = Accelerator()\n",
        "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
        "    model, optimizer, train_dataloader, eval_dataloader\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNvsSTqgRhmw"
      },
      "outputs": [],
      "source": [
        "from transformers import get_scheduler\n",
        "\n",
        "num_train_epochs = 3\n",
        "num_update_steps_per_epoch = len(train_dataloader)\n",
        "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OllIeJ3pRhmw",
        "outputId": "d667d81d-76e8-4be6-8574-52cf6b84c8f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'sgugger/marian-finetuned-kde4-en-to-fr-accelerate'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from huggingface_hub import Repository, get_full_repo_name\n",
        "\n",
        "model_name = \"marian-finetuned-kde4-en-to-fr-accelerate\"\n",
        "repo_name = get_full_repo_name(model_name)\n",
        "repo_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lw3NTd_4Rhmw"
      },
      "outputs": [],
      "source": [
        "output_dir = \"marian-finetuned-kde4-en-to-fr-accelerate\"\n",
        "repo = Repository(output_dir, clone_from=repo_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training loop\n",
        "We are now ready to write the full training loop. To simplify its evaluation part, we define this `postprocess()` function that takes predictions and labels and converts them to the lists of strings our metric object will expect:"
      ],
      "metadata": {
        "id": "mKqjaXUWfn8k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpGdKehfRhmw"
      },
      "outputs": [],
      "source": [
        "def postprocess(predictions, labels):\n",
        "    predictions = predictions.cpu().numpy()\n",
        "    labels = labels.cpu().numpy()\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
        "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
        "    return decoded_preds, decoded_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training loop looks a lot like the ones in section 2 and Chapter 3, with a few differences in the evaluation part ‚Äî so let‚Äôs focus on that!\n",
        "\n",
        "- The first thing to note is that we use the `generate()` method to compute predictions, but this is a method on our base model, not the wrapped model ü§ó Accelerate created in the `prepare()` method. That‚Äôs why we unwrap the model first, then call this method.\n",
        "\n",
        "- The second thing is that, like with **token classification**, two processes may have padded the inputs and labels to different shapes, so we use `accelerator.pad_across_processes()` to make the predictions and labels the same shape before calling the `gather()` method. If we don‚Äôt do this, the evaluation will either error out or hang forever."
      ],
      "metadata": {
        "id": "k4iQ8j0AfwEp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8pqvbqXRhmw",
        "outputId": "e625b2a8-447d-460e-d3df-bb8ce279de43"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "epoch 0, BLEU score: 53.47\n",
              "epoch 1, BLEU score: 54.24\n",
              "epoch 2, BLEU score: 54.44"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "for epoch in range(num_train_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    for batch in train_dataloader:\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        accelerator.backward(loss)\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    for batch in tqdm(eval_dataloader):\n",
        "        with torch.no_grad():\n",
        "            generated_tokens = accelerator.unwrap_model(model).generate(\n",
        "                batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"],\n",
        "                max_length=128,\n",
        "            )\n",
        "        labels = batch[\"labels\"]\n",
        "\n",
        "        # Necessary to pad predictions and labels for being gathered\n",
        "        generated_tokens = accelerator.pad_across_processes(\n",
        "            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
        "        )\n",
        "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
        "\n",
        "        predictions_gathered = accelerator.gather(generated_tokens)\n",
        "        labels_gathered = accelerator.gather(labels)\n",
        "\n",
        "        decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)\n",
        "        metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
        "\n",
        "    results = metric.compute()\n",
        "    print(f\"epoch {epoch}, BLEU score: {results['score']:.2f}\")\n",
        "\n",
        "    # Save and upload\n",
        "    accelerator.wait_for_everyone()\n",
        "    unwrapped_model = accelerator.unwrap_model(model)\n",
        "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
        "    if accelerator.is_main_process:\n",
        "        tokenizer.save_pretrained(output_dir)\n",
        "        repo.push_to_hub(\n",
        "            commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once this is done, you should have a model that has results pretty similar to the one trained with the `Seq2SeqTrainer`. You can check the one we trained using this code at huggingface-course/`marian-finetuned-kde4-en-to-fr-accelerate`. And if you want to test out any tweaks to the training loop, you can directly implement them by editing the code shown above!"
      ],
      "metadata": {
        "id": "9SE85QrsgBfn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using the fine-tuned model\n",
        "We‚Äôve already shown you how you can use the model we fine-tuned on the Model Hub with the inference widget. To use it locally in a pipeline, we just have to specify the proper model identifier:"
      ],
      "metadata": {
        "id": "BsyS1HNcgKx6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSZNfItjRhmw",
        "outputId": "9d01af04-18e4-4708-f66a-f0ce19bd8538"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'translation_text': 'Par d√©faut, d√©velopper les fils de discussion'}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Replace this with your own checkpoint\n",
        "model_checkpoint = \"huggingface-course/marian-finetuned-kde4-en-to-fr\"\n",
        "translator = pipeline(\"translation\", model=model_checkpoint)\n",
        "translator(\"Default to expanded threads\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoYFHIf5Rhmw",
        "outputId": "643b48a9-3ffe-42c9-f92c-5938bcf0f3cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'translation_text': \"Impossible d'importer %1 en utilisant le module externe d'importation OFX. Ce fichier n'est pas le bon format.\"}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translator(\n",
        "    \"Unable to import %1 using the OFX importer plugin. This file is not the correct format.\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Translation (PyTorch)",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}