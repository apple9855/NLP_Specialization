{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1GpnoJSSGee"
      },
      "source": [
        "# Understanding the Interface class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmRG85m3SGef"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJ9WetWWSGeg"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to create an Interface\n",
        "You‚Äôll notice that the `Interface` class has 3 required parameters:\n",
        "\n",
        "Interface(fn, inputs, outputs, ...)\n",
        "\n",
        "These parameters are:\n",
        "\n",
        "- fn: the prediction function that is wrapped by the Gradio interface. This function can take one or more parameters and return one or more values\n",
        "- inputs: the input component type(s). Gradio provides many pre-built components such as\"image\" or \"mic\".\n",
        "- outputs: the output component type(s). Again, Gradio provides many pre-built components e.g. \"image\" or \"label\".\n",
        "\n",
        "\n",
        "For a complete list of components, see the [Gradio docs ](https://www.gradio.app/docs). Each pre-built component can be customized by instantiating the class corresponding to the component.\n",
        "\n",
        "- For example, as we saw in the previous section, instead of passing in \"textbox\" to the inputs parameter, you can pass in a `Textbox(lines=7, label=\"Prompt\")` component to create a textbox with 7 lines and a label."
      ],
      "metadata": {
        "id": "QE6LzewASOQW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A simple example with audio\n",
        "As mentioned earlier, Gradio provides many different inputs and outputs. So let‚Äôs build an `Interface that works with audio`.\n",
        "\n",
        "In this example, we‚Äôll build an `audio-to-audio` function that takes an audio file and simply reverses it.\n",
        "\n",
        "We will use for the input the `Audio component`. When using the Audio component, you can specify whether you want the source of the audio to be a file that the user uploads or a microphone that the user records their voice with. In this case, let‚Äôs set it to a `\"microphone\"`. Just for fun, we‚Äôll add a label to our Audio that says `‚ÄúSpeak here‚Ä¶‚Äú`.\n",
        "\n",
        "In addition, we‚Äôd like to receive the audio as a numpy array so that we can easily ‚Äúreverse‚Äù it. So we‚Äôll set the \"type\" to be \"numpy\", which passes the input data as a tuple of (sample_rate, data) into our function.\n",
        "\n",
        "We will also use the Audio output component which can automatically render a tuple with a sample rate and numpy array of data as a playable audio file. In this case, we do not need to do any customization, so we will use the string shortcut \"audio\"."
      ],
      "metadata": {
        "id": "CoXI755VSzih"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-m-SneLSGeh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gradio as gr\n",
        "\n",
        "\n",
        "def reverse_audio(audio):\n",
        "    sr, data = audio\n",
        "    reversed_audio = (sr, np.flipud(data))\n",
        "    return reversed_audio\n",
        "\n",
        "\n",
        "mic = gr.Audio(source=\"microphone\", type=\"numpy\", label=\"Speak here...\")\n",
        "gr.Interface(reverse_audio, mic, \"audio\").launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling multiple inputs and outputs\n",
        "Let‚Äôs say we had a more complicated function, with multiple inputs and outputs. In the example below, we have a function that takes a dropdown index, a slider value, and number, and returns an audio sample of a musical tone.\n",
        "\n",
        "Take a look how we pass a list of input and output components, and see if you can follow along what‚Äôs happening.\n",
        "\n",
        "The key here is that when you pass:\n",
        "\n",
        "- a list of input components, each component corresponds to a parameter in order.\n",
        "- a list of output coponents, each component corresponds to a returned value.\n",
        "The code snippet below shows how three input components line up with the three arguments of the `generate_tone()` function:"
      ],
      "metadata": {
        "id": "I1iPSO1zTOGY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Qvm_eXgSGeh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gradio as gr\n",
        "\n",
        "notes = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]\n",
        "\n",
        "\n",
        "def generate_tone(note, octave, duration):\n",
        "    sr = 48000\n",
        "    a4_freq, tones_from_a4 = 440, 12 * (octave - 4) + (note - 9)\n",
        "    frequency = a4_freq * 2 ** (tones_from_a4 / 12)\n",
        "    duration = int(duration)\n",
        "    audio = np.linspace(0, duration, duration * sr)\n",
        "    audio = (20000 * np.sin(audio * (2 * np.pi * frequency))).astype(np.int16)\n",
        "    return (sr, audio)\n",
        "\n",
        "\n",
        "gr.Interface(\n",
        "    generate_tone,\n",
        "    [\n",
        "        gr.Dropdown(notes, type=\"index\"),\n",
        "        gr.Slider(minimum=4, maximum=6, step=1),\n",
        "        gr.Textbox(type=\"number\", value=1, label=\"Duration in seconds\"),\n",
        "    ],\n",
        "    \"audio\",\n",
        ").launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The launch() method\n",
        "So far, we have used the `launch()` method to launch the interface, but we haven‚Äôt really discussed what it does.\n",
        "\n",
        "By default, the `launch() `method will launch the demo in a web server that is running locally. If you are running your code in a Jupyter or Colab notebook, then Gradio will embed the demo GUI in the notebook so you can easily use it.\n",
        "\n",
        "You can customize the behavior of `launch() `through different parameters:\n",
        "\n",
        "- inline - whether to display the interface inline on Python notebooks.\n",
        "- inbrowser - whether to automatically launch the interface in a new tab on the default browser.\n",
        "- share - whether to create a publicly shareable link from your computer for the interface. Kind of like a Google Drive link!"
      ],
      "metadata": {
        "id": "G26ymy4zTfyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úèÔ∏è Let‚Äôs apply it!\n",
        "Let‚Äôs build an interface that allows you to demo a `speech-recognition` model. To make it interesting, we will accept either a mic input or an uploaded file.\n",
        "\n",
        "As usual, we‚Äôll load our speech recognition model using the `pipeline()` function from ü§ó Transformers. If you need a quick refresher, you can go back to that section in Chapter 1.\n",
        "\n",
        "Next, we‚Äôll implement a `transcribe_audio()` function that processes the audio and returns the transcription.\n",
        "\n",
        "Finally, we‚Äôll wrap this function in an Interface with the Audio components for the inputs and just text for the output. Altogether, the code for this application is the following:"
      ],
      "metadata": {
        "id": "Vs9NkHXLTvjv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKMKLVN_SGeh"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import gradio as gr\n",
        "\n",
        "model = pipeline(\"automatic-speech-recognition\")\n",
        "\n",
        "\n",
        "def transcribe_audio(mic=None, file=None):\n",
        "    if mic is not None:\n",
        "        audio = mic\n",
        "    elif file is not None:\n",
        "        audio = file\n",
        "    else:\n",
        "        return \"You must either provide a mic recording or a file\"\n",
        "    transcription = model(audio)[\"text\"]\n",
        "    return transcription\n",
        "\n",
        "\n",
        "gr.Interface(\n",
        "    fn=transcribe_audio,\n",
        "    inputs=[\n",
        "        gr.Audio(source=\"microphone\", type=\"filepath\", optional=True),\n",
        "        gr.Audio(source=\"upload\", type=\"filepath\", optional=True),\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        ").launch()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Understanding the Interface class",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}